{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Forward Pass\n",
    "\n",
    "Having completed work on the symmetry function calculation, despite the disappointing overall timing, we can move on to the forward pass, written in a way that either the individual or total energies can be calculated given the input parameters and the symmetry functions. This either means passing individual vectors, or a full matrix, though passing a matrix is a question for Alex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "@everywhere using StaticArrays,BenchmarkTools,Random,DelimitedFiles,LinearAlgebra\n",
    "@everywhere using InvertedIndices\n",
    "@everywhere using MachineLearningPotential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a _struct_ containing the actual neural network parameters. In general this should contain the integer num_layers as well as vectors of length num_layers num_nodes and activation_functions. We use the num_nodes vector to define the num_parameters and this gives us the vector length for parameters, the actual values contained in the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkPotential"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct NeuralNetworkPotential\n",
    "    n_layers::Int32\n",
    "    n_params::Int32\n",
    "    num_nodes::Vector\n",
    "    activation_functions::Vector\n",
    "    parameters::Vector\n",
    "end\n",
    "\n",
    "function NeuralNetworkPotential(num_nodes::Vector,activation_functions::Vector, parameters)\n",
    "    \n",
    "    return NeuralNetworkPotential(length(num_nodes),length(parameters),num_nodes,activation_functions,parameters)\n",
    "end\n",
    "\n",
    "# function forward_pass(eatom,input,batchsize,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_pass (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_pass( input::AbstractArray, batchsize, num_layers, num_nodes, activation_functions, num_parameters, parameters)    \n",
    "    eatom = Vector{Float64}(undef,batchsize)\n",
    "    ccall( (:forward, \"./librunnerjulia.so\"),\n",
    "    Float64,  (Ref{Float64},Ref{Int32}, Ref{Int32}, Ref{Int32}, Ref{Int32},\n",
    "    Ref{Int32}, Ref{Float64}, Ref{Int32}, Ref{Float64}),\n",
    "    input, num_nodes[1], num_layers, num_nodes, batchsize,\n",
    "    num_parameters, parameters, activation_functions, eatom\n",
    "    )\n",
    "    return eatom\n",
    "end\n",
    "\n",
    "function forward_pass(input::AbstractArray,batchsize,nnparams::NeuralNetworkPotential)   \n",
    "    return forward_pass(input, batchsize, nnparams.n_layers, nnparams.num_nodes, nnparams.activation_functions,nnparams.n_params, nnparams.parameters)\n",
    "end\n",
    "\n",
    "function forward_pass(eatom,input::AbstractArray,batchsize,nnparams::NeuralNetworkPotential)   \n",
    "    return forward_pass(eatom,input, batchsize, nnparams.n_layers, nnparams.num_nodes, nnparams.activation_functions,nnparams.n_params, nnparams.parameters)\n",
    "end\n",
    "\n",
    "function forward_pass( eatom,input::AbstractArray, batchsize, num_layers, num_nodes, activation_functions, num_parameters, parameters)    \n",
    "    ccall( (:forward, \"./librunnerjulia.so\"),\n",
    "    Float64,  (Ref{Float64},Ref{Int32}, Ref{Int32}, Ref{Int32}, Ref{Int32},\n",
    "    Ref{Int32}, Ref{Float64}, Ref{Int32}, Ref{Float64}),\n",
    "    input, num_nodes[1], num_layers, num_nodes, batchsize,\n",
    "    num_parameters, parameters, activation_functions, eatom\n",
    "    )\n",
    "    return eatom\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkPotential(4, 2221, Int32[88, 20, 20, 1], Int32[1, 2, 2, 1], [-1.0768943782, 0.3563458393, -1.3084861447, 0.1127640916, -0.5646591931, -0.4969830793, 1.423770514, -0.2005662393, -0.8957859374, 0.6076110858  â€¦  0.1939524848, -0.0342903191, -0.0231042009, 0.0718481968, -0.0828320122, -0.0728501885, 0.0306628826, 0.0486053813, -0.0867018862, 0.0441651841])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_nodes::Vector{Int32} = [88, 20, 20, 1]\n",
    "activation_functions::Vector{Int32} = [1, 2, 2, 1]\n",
    "file = open(\"weights.029.data\",\"r+\")\n",
    "weights=readdlm(file)\n",
    "close(file)\n",
    "weights = vec(weights)\n",
    "gauge_thing = NeuralNetworkPotential(num_nodes,activation_functions,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"$(pwd())/symfunctions.out\",\"r+\")\n",
    "truevalues = readdlm(file)\n",
    "close(file)\n",
    "truevals = transpose(truevalues[2:end,2:end])\n",
    "truevals = Matrix{Float64}(truevals)\n",
    "\n",
    "file = open(\"$(pwd())/scaling.data\")\n",
    "scalingvalues = readdlm(file)\n",
    "close(file)\n",
    "G_value_vec = []\n",
    "for row in eachrow(scalingvalues[1:88,:])\n",
    "    max_min = [row[4],row[3]]\n",
    "    push!(G_value_vec,max_min)\n",
    "end\n",
    "renormalised_truevals = copy(truevals)\n",
    "for g_index in eachindex(G_value_vec)\n",
    "    renormalised_truevals[g_index,:] .-=  G_value_vec[g_index][2]\n",
    "    renormalised_truevals[g_index,:] = renormalised_truevals[g_index,:] ./ (G_value_vec[g_index][1] - G_value_vec[g_index][2])\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Forward Pass\n",
    "writing the forward pass function, time to see whether it returns the correct energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.216814248768345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E = forward_pass(renormalised_truevals,55,gauge_thing)\n",
    "sum(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.216814248768345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E = forward_pass(E,renormalised_truevals,55,gauge_thing)\n",
    "sum(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/ghunter/.julia/dev/MachineLearningPotential/scripts/\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"$(pwd())/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
